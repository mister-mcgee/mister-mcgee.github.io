---
title: Binary, Bits and Bytes
author: Mister McGee
tags: ["csf", "binary", "programming"]
draft: true
---

import Define    from "@components/learn/Define.astro";
import Wikipedia from "@components/learn/Wikipedia.astro";

## What Is Binary

The word <Define term="binary"/> comes from the latin *bini-* (meaning *two*), and *-ary* (meaning *part of*). The word *binary* describes something with two parts, but in the language of mathematics and computer science it refers to the base-2 number system where all numerals are expressed using the digits zero `0` and one `1`.

It can be intimidating to approach new mathematical concepts, but before your eyes glaze over I encourage you to persevere. Understanding binary is fundamental in understanding modern computing.

While base-2 may seem foreign, you are likely already accustomed to using the base-10 numeric system known as <Define term="decimal"/>. Thinking about how we interpret decimal numerals can give us some insight regarding how we should interpret binary numerals.

In base-10 each numeral is represented using *ten* digits, from zero `0` to nine `9`. Each digit and its *place* within a numeral decide its value. Consider the base-10 numeral `345` where we describe (from right to left) the digit five `5` as being in the *ones place*, the digit four `4` in the *tens place*, and the digit three `3` in the *hundreds place*. Those practiced in reading decimal numerals know this means there are five *ones* (`5`), four *tens* (`40`), and three *hundreds* (`300`). Another way to visualize this relationships is to write the numeral in its *expanded form* -

$$\text{``345''} = (3 \times 10^2) + (4 \times 10^1) + (5 \times 10^0)$$

$$\text{``345''} = (3 \times 100 ) + (4 \times 10  ) + (5 \times 1   )$$

$$\text{``345''} = 300 + 40 + 5 = 345$$

When looking at a decimal numeral's *expanded form* it is easy to see that each *place* represents a power of ten `10`. We can use a similar expanded form to describe binary numerals, but each *place* represents a power of two `2` instead. Consider the binary numeral `1010`, (from right to left) we would say that the digit zero `0` is in the *ones place*, the digit one `1` is in the *twos place*, the digit zero `0` is in the *fours place* and the digit one `1` is in the *eights place*. We can write the numeral in its *expanded form* as -


$$\text{``1010''} = (1 \times 2^3) + (0 \times 2^2) + (1 \times 2^1) + (0 \times 2^0)$$

$$\text{``1010''} = ( 1 \times 8 ) + ( 0 \times 4 ) + ( 1 \times 2 ) + ( 0 \times 1 )$$

$$\text{``1010''} = 8 + 0 + 2 + 0 = 10$$


We can sum each part of our expanded form to see that the binary numeral `1010` is equal to the decimal numeral `10`. While it's outside the scope of this lesson, programmers often use several different bases to represent numerals in code. Most programming languages support the use of <Define term="binary"/> (base-2), <Define term="octal"/> (base-8), <Define term="decimal"/> (base-10), and <Define term="hexadecimal"/> (base-16) numerals.

Take some time to play with the binary to decimal converter below to get a feel for how binary counting works.

<a id="binary-to-decimal"></a>

import BinaryToDecimal from "@components/fun/BinaryToDecimal";

<BinaryToDecimal client:only/>

## Why Use Binary

A computer is a "thinking" machine, meaning that it is capable of "choosing" an outcome based on some input or prior state. These decisions are made using physical components called *switches*, which control the flow of electricity. In a modern electronic computer, binary signals are transmitted by encoding zeroes `0` as "low" voltage and ones `1` as "high" voltage, but this wasn't always the case.

Early electronic and electromechanical computers explored a variety of encodings, including trinary and bi-quinary systems, which could represent more than two states with a single switch. These approaches reduced the total number of switches required, making them attractive at a time when switches were bulky, power-hungry, and expensive.

Despite these early alternatives, binary has always been appealing to engineers and computer scientists alike because of its simplicity. Intuitively, the circuitry required to distinguish between *two* voltages is relatively simple when compared to the circuitry required to distinguish between *many* voltages, especially in noisy electrical environments.

The turning point came with the invention of the <Wikipedia term="transistor">transistor</Wikipedia> in 1947. Transistors replaced vacuum tubes and mechanical relays with smaller, cheaper, and far more efficient components. Suddenly, the trade-off that once favored complex encodings no longer made sense. With transistors, binary systems became not only simpler to build, but also cheaper and faster — setting the stage for the modern digital age.

## Bits and Bytes

A <Define term="bit"/> (short for *binary digit*) is the smallest unit of *information*. A bit can represent one of two states - zero `0` or one `1`, *no* or *yes*, *false* or *true*, and so on. While bits are important, it is often not useful to think or speak about them individually. Instead, computer scientists learn to think in <Define term="byte">bytes</Define>, since most modern computer architectures operate on groupings of 8 bits (aka *bytes*) at a time.

While a single bit is capable of representing only two states (e.g. *off* or *on*), a *byte* is capable of representing up to 256 states! If we imagine for a moment, that a bit is like a light switch, then we can think of each *state* as some unique combination of *offs* and *ons*. Play with the visualizer below to get a feel for how adding bits to a binary sequence affects the number of representable states or combinations.

import BinaryCombinations from "@components/fun/BinaryCombinations.tsx";

<BinaryCombinations client:only/>

<span className="text-sm italic">This visualization uses the <Wikipedia term="hamming_distance">hamming distance</Wikipedia> of each binary numeral to color and connect it to its hamming-adjacent neighbors.</span>

If it's not already obvious, each additional *bit* in a binary sequence *doubles* the number of states that can be represented. While a single byte can represent up to 256 states (usually a number between `0` and `255` inclusive), it is common to use *multiple bytes* to represent larger or more complicated numbers. Computers represent numbers in many ways, but for the sake of simplicity, lets examine how computers represent *integers*.

An *integer* is a whole number (including zero `0`) that can be *signed* (positive or negative) or *unsigned*. Most integer arithmetic on computers is performed using 32-bit (4 byte) or 64-bit (8 byte) integers. Depending on the *width* (number of bits) and whether it is signed or unsigned, an integer can represent a range of whole numbers. Take a look at the table below to see how different combinations of width and signedness affect the range of representable values.

<div className="text-sm">
| Width | Signed? | Minimum | Maximum |
| --- | --- | --- | --- |
| 32-bit (4 bytes) | Yes | $$-2,147,483,648$$ | $$2,147,483,647$$ |
| 32-bit (4 bytes) | No  | $$0$$ | $$4,294,967,295$$ |
| 64-bit (8 bytes) | Yes | $$-9,223,372,036,854,775,808$$ | $$9,223,372,036,854,775,807$$ |
| 64-bit (8 bytes) | No  | $$0$$ | $$18,446,744,073,709,551,615$$ |
</div>

This raises an important consideration in software development: computers, although scalable, are still finite machines. When programmers need to work with numbers, they must choose a data type with a specific *width* and *signedness* that fits their needs. For instance, consider a video game where a player starts at level one `1` and can progress up to level one-hundred `100`. Although any integer type could represent this range, a programmer might opt to use an unsigned byte. Since negative values aren't needed and 100 is well within the byte's maximum value of 255.

## Interpreting Binary

If you were presented with a sequence of binary numbers and asked to *interpret* them, you might be justifiably confused. Binary data by itself has no inherent meaning — it only becomes meaningful once we decide *how* to interpret it. Sometimes this decision is made by an individual programmer within their own codebase, but more often, it's defined by widely adopted *standards*.

Standards are detailed technical documents that describe how data should be structured, transmitted, or processed. Hardware and software vendors implement these standards to ensure compatibility across products and platforms. For example, an NVIDIA or AMD graphics card can work seamlessly with an ASUS motherboard because all three companies follow the same underlying standards — even if they don't share internal design details with one another.

There are many well-known standards that govern different aspects of hardware and software. A few notable examples include -

- The "IEEE 754 - Standard for Binary Floating-Point Arithmetic" which describes how to represent and perform arithmetic on floating-point (fractional) numbers. 
- The W3C Consortium standards for *HTML*, *CSS*, and *JS* which outline the features and syntax of these languages, while also describing how they should be interpreted by browsers and other engines.
- The "RISC-V Instruction Set Manual" which describes how a RISC-V processor should interpret and execute binary instructions.

Standard CPU architectures like RISC-V, x86, and ARM are what tell a computer how to interpret and execute binary instructions. It is up to the CPU manufacturer to design compatible hardware, and it is up to application developers to write compatible software according to the standard.

## The Language of Computing

Computers speak only one language — and it isn't English. It's also not JavaScript, Python, or any other programming language you might recognize. Fundamentally, computers “think” in binary. Every program — whether it's a simple calculator, a web app, or a AAA video game — is ultimately a sequence of binary instructions executed by the computer's hardware.

Although computers "think" in binary, writing programs in binary is both tedious and error-prone for humans. This is where programming languages fit into the picture.

A programming language is a human-friendly way of writing instructions for a computer. Languages like Python or Javascript allow us to describe what we want a computer to do using words, symbols, and abstractions humans can understand. These languages are tools made *by humans for humans* — designed to make it easier to reason about and build complex software systems.

You may wonder why programmers don't write software using plain English. The problem is that natural languages are inherently ambiguous — words can have multiple meanings, and sentences can be interpreted in different ways depending on context. An author's favorite cliche is double-entendre, which happens to be a programmer's worst nightmare. Computers require precise and unambiguous instructions. Thinking and writing in "computer speak" might feel awkward to a beginner, but it's important to realize that a programming language is designed to *eliminate ambiguity*, not necessarily to mirror natural language.

After a developer has written a program using a programming language it must then be translated into the relevant binary instructions by specialized tools. Broadly speaking these tools fall into a few categories, including but not limited to - compilers, interpreters, and virtual machines, all of which translate human-readable code into machine-readable instructions.

Programming languages exist to bridge the gap between human thought and machine execution. They allow engineers and programmers to focus on solving problems, building systems, and creating experiences — without needing to think in zeros and ones.

## Summary

Binary is the foundational language of computers, representing data using only two digits: zero `0` and one `1`. While this base-2 system may seem unfamiliar compared to the base-10 decimal system we use every day, understanding binary is essential to grasping how modern computers operate.

Computers use electrical signals, often represented as low and high voltages, to encode binary data. Although early computers experimented with other encoding methods, the invention of the transistor enabled binary to become the dominant standard due to its simplicity and efficiency.

Information is stored in bits—the smallest unit of information—and grouped into bytes (8 bits), which can represent many possible values. Larger numbers and more complex data types are built by combining multiple bytes.

Binary data alone is meaningless without a way to interpret it. This interpretation is guided by standards that ensure hardware and software from different manufacturers can work together. CPU architectures and industry standards define how binary instructions are executed and how data is formatted.

Though computers “think” in binary, humans write programs using programming languages like Python or JavaScript. These languages act as tools created by humans for humans to communicate instructions clearly and unambiguously. Natural languages like English are too ambiguous for precise instructions, so programming languages eliminate that ambiguity, even if their syntax can feel awkward at first.

Programs written in human-friendly languages are translated into binary instructions by compilers, interpreters, or virtual machines. This process bridges the gap between human ideas and machine execution, allowing developers to build software without needing to think directly in zeros and ones.

## Review

import Quiz from "@components/learn/Quiz.astro";
import quiz from "@learn/csf/week/02/quiz.json";

<Quiz quiz={quiz} title={frontmatter.title}/>

## Project

*Thinking Like a Programmer*

#### Objectives



With your partner, design a binary instruction set

import Rubric from "@components/learn/Rubric.astro";
import rubric from "@learn/csf/week/02/rubric.json";

<Rubric rubric={rubric} title="Thinking Like a Programmer"/>

## Glossary

import Glossary from "@components/learn/Glossary.astro";

<Glossary/>





